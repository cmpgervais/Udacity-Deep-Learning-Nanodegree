{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Multi-Layer Perceptron TensorFlow Tutorial\n",
    "\n",
    "This is a more basic version of the Intro to TensorFlow tutorial in that it doesn't use definitions to build the graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the MNIST data set and saving it to the object ```mnist```. \n",
    "\n",
    "**Warning:** This will throw an error that it's deprecated, don't worry about that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-12a488ac486e>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting .\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting .\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting .\\t10k-images-idx3-ubyte.gz\n",
      "Extracting .\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot = True, reshape = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up hyperparameters\n",
    "\n",
    "Note that ```n_input``` and ```n_classes``` aren't really hyperparameters here, as they're given by the dimensions and characteristics of the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_classes = 10\n",
    "display_step = 1\n",
    "\n",
    "learning_rate = 0.0005\n",
    "training_epochs = 30\n",
    "batch_size = 128\n",
    "keep_prob = 1\n",
    "\n",
    "n_hidden_layer_1 = 256\n",
    "n_hidden_layer_2 = 512\n",
    "n_hidden_layer_3 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're going to set up a dictionary of weights and biases for the hidden layer and output layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'hidden_layer_1': tf.Variable(tf.random_normal([n_input, n_hidden_layer_1])),\n",
    "    'hidden_layer_2': tf.Variable(tf.random_normal([n_hidden_layer_1, n_hidden_layer_2])),\n",
    "    'hidden_layer_3': tf.Variable(tf.random_normal([n_hidden_layer_2, n_hidden_layer_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer_3, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer_1': tf.Variable(tf.random_normal([n_hidden_layer_1])),\n",
    "    'hidden_layer_2': tf.Variable(tf.random_normal([n_hidden_layer_2])),\n",
    "    'hidden_layer_3': tf.Variable(tf.random_normal([n_hidden_layer_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the graph input layer using placeholders! What is the purpose of the ```None``` here? Because the MNIST dataset is already split into batches, and because the batch sizes won't always be the same size (there will be one leftover batch that is not sized the same as the others), we use ```None``` as the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an input layer, we can add the first hidden layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer_1']), biases['hidden_layer_1'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer_2']), biases['hidden_layer_2'])\n",
    "layer_2 = tf.nn.relu(layer_2)\n",
    "layer_2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "\n",
    "layer_3 = tf.add(tf.matmul(layer_2, weights['hidden_layer_3']), biases['hidden_layer_3'])\n",
    "layer_3 = tf.nn.relu(layer_3)\n",
    "layer_3 = tf.nn.dropout(layer_3, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the logits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.add(tf.matmul(layer_3, weights[\"out\"]), biases[\"out\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 65.485839844\n",
      "Epoch: 0002 cost= 32.047142029\n",
      "Epoch: 0003 cost= 10.172771454\n",
      "Epoch: 0004 cost= 17.096355438\n",
      "Epoch: 0005 cost= 32.638824463\n",
      "Epoch: 0006 cost= 39.381332397\n",
      "Epoch: 0007 cost= 49.191001892\n",
      "Epoch: 0008 cost= 9.111293793\n",
      "Epoch: 0009 cost= 31.036819458\n",
      "Epoch: 0010 cost= 12.819181442\n",
      "Epoch: 0011 cost= 5.583021164\n",
      "Epoch: 0012 cost= 17.609828949\n",
      "Epoch: 0013 cost= 0.000000000\n",
      "Epoch: 0014 cost= 8.453992844\n",
      "Epoch: 0015 cost= 11.057997704\n",
      "Epoch: 0016 cost= 0.005510075\n",
      "Epoch: 0017 cost= 6.143325806\n",
      "Epoch: 0018 cost= 0.000000017\n",
      "Epoch: 0019 cost= 10.841690063\n",
      "Epoch: 0020 cost= 0.023333196\n",
      "Epoch: 0021 cost= 8.689037323\n",
      "Epoch: 0022 cost= 9.242700577\n",
      "Epoch: 0023 cost= 0.000000000\n",
      "Epoch: 0024 cost= 4.138766289\n",
      "Epoch: 0025 cost= 2.031311035\n",
      "Epoch: 0026 cost= 1.176554441\n",
      "Epoch: 0027 cost= 2.230530262\n",
      "Epoch: 0028 cost= 0.000000000\n",
      "Epoch: 0029 cost= 0.000000000\n",
      "Epoch: 0030 cost= 4.999013901\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(c))\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # Decrease test_size if you don't have enough memory\n",
    "    test_size = 256\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:test_size], y: mnist.test.labels[:test_size]}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
