{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with TensorFlow!\n",
    "\n",
    "The following tutorial has been modified from the \"Introduction to TensorFlow\" from the Udacity Deep Learning Nanodegree. We start by importing the usual suspect modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow, data doesn't get stored as integers, floats, strings etc. Rather, these values are stored in objects called tensors (hence the module name!) In the example below, the number 1 is an int32 tensor with 0 dimensions, while the list 1:4 is an int32 tensor with 1 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "dim_zero_constant = tf.constant(1)\n",
    "dim_one_constant = tf.constant([1,2,3,4])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    dim_zero_output = sess.run(dim_zero_constant)\n",
    "    dim_one_output = sess.run(dim_one_constant)\n",
    "    print(dim_zero_output)\n",
    "    print(dim_one_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow API is constructed around this idea of a computational graph, which is just a way of conceptualizing a mathematical process. Our job is to build the graph, which then gets run as a \"TensorFlow Session\" by calling ```tf.Session()``` and running via ```sess.run()```. Essentially, the session is in charge of allocating the processes to the CPU or GPU depending on your setup.\n",
    "\n",
    "<img src=\"image/computational_graph.png\" style=\"height: 75%;width: 75%; position: relative; right: 5%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's great.. but quite often you'll be working with tensors that aren't constant. For this you use ```tf.placeholder()``` in combination with ```feed_dict()```. When I first started with TensorFlow - which admittedly was **not** very long ago, so trust me when I say that we're probably in the same boat - I found them quite confusing. Try not to overthink this, placeholders are nothing more than a variable that we will assing data to later on in the code. It allows us to build up our operations and create the computational graph without needing the data immediately. We then pass the placeholder a dictionary of data, or in TensorFlow terms, we *feed a data dictionary to the placeholder*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's basically it. So in the example below, we tell TensorFlow that ```x``` will at some point in the future be of type ```tf.string```. Then when we run the session, we feed the placeholder a data dictionary - \"Hello World\" - and that's that! This data can then be used to perform whatever operation is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict = {x: \"Hello World!\"})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving beyond the basics\n",
    "\n",
    "Now let's get to the heart of TensorFlow - building neural networks. We've already seen ```tf.constant()``` and ```tf.placeholder()``` objects, but these can't be directly modified, and thus aren't appropriate if you want to update weights and biases like you would when creating neural networks. This is where ```tf.Variable()``` comes in to play. Now, if you're like me and aren't super familiar or comfortable with building your scripts up from definitions, don't fret. Take some time to read through the definition descriptions below and it'll all make sense shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(n_features, n_labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \n",
    "    Notes:\n",
    "    tf.truncated_nromal() returns a tensor with random values from a normal distribution\n",
    "    \"\"\"\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Nunmber of labels\n",
    "    :return: TensorFlow bias\n",
    "    \n",
    "    Notes:\n",
    "    tf.Variable() creates a tesnor with an initial value that can be modified, much like a Python variable\n",
    "    tf.zeros() returns a tensor with all zeros\n",
    "    We can set the bias to zero because the randomized weights already prevent the model from getting stuck\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input, ie x variable\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow bias\n",
    "    :return: TensorFlow linear combination equal to xw + b\n",
    "    \n",
    "    Notes:\n",
    "    tf.matmul() takes the matrix multiplication of two matrices - don't forget order matters!\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first n labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \n",
    "    Notes:\n",
    "    read_data_sets is deprecated and will be removed in the future!\n",
    "    \"\"\"\n",
    "    \n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "    \n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot = True)\n",
    "    \n",
    "    # In order to make this run faster, only look at first 10,000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "        \n",
    "        # Only add features and labels if it's for the first n labels - note n can be 1 to 10\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "            \n",
    "    return mnist_features, mnist_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-07f590b01cd6>:14: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Chris Gervais\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Number of features is 28 x 28 image = 784 features ie one feature for each pixel\n",
    "n_features = 784\n",
    "\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# Weights and biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear combination xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 10.412758827209473\n",
      "Loss = 3.8213024139404297\n",
      "Loss = 2.5183959007263184\n",
      "Loss = 1.9152885675430298\n",
      "Loss = 1.7444946765899658\n",
      "Loss = 1.0473048686981201\n",
      "Loss = 0.855830729007721\n",
      "Loss = 0.6647284030914307\n",
      "Loss = 0.5960330367088318\n",
      "Loss = 0.5474634766578674\n",
      "Loss = 0.5090176463127136\n",
      "Loss = 0.47640350461006165\n",
      "Loss = 0.4482499957084656\n",
      "Loss = 0.4237080216407776\n",
      "Loss = 0.4021487236022949\n",
      "Loss = 0.3830606937408447\n",
      "Loss = 0.36602139472961426\n",
      "Loss = 0.3506879210472107\n",
      "Loss = 0.3367922902107239\n",
      "Loss = 0.32413098216056824\n",
      "Loss = 0.3125483989715576\n",
      "Loss = 0.301920622587204\n",
      "Loss = 0.2921425700187683\n",
      "Loss = 0.28312066197395325\n",
      "Loss = 0.27477189898490906\n",
      "Loss = 0.2670232653617859\n",
      "Loss = 0.25981199741363525\n",
      "Loss = 0.25308412313461304\n",
      "Loss = 0.24679362773895264\n",
      "Loss = 0.240900456905365\n",
      "Loss = 0.23536986112594604\n",
      "Loss = 0.23017048835754395\n",
      "Loss = 0.22527460753917694\n",
      "Loss = 0.22065706551074982\n",
      "Loss = 0.21629507839679718\n",
      "Loss = 0.2121679186820984\n",
      "Loss = 0.20825672149658203\n",
      "Loss = 0.204544335603714\n",
      "Loss = 0.20101504027843475\n",
      "Loss = 0.197654590010643\n",
      "Loss = 0.19444990158081055\n",
      "Loss = 0.19138918817043304\n",
      "Loss = 0.18846164643764496\n",
      "Loss = 0.1856575906276703\n",
      "Loss = 0.18296806514263153\n",
      "Loss = 0.18038512766361237\n",
      "Loss = 0.1779015064239502\n",
      "Loss = 0.1755104660987854\n",
      "Loss = 0.17320606112480164\n",
      "Loss = 0.17098292708396912\n",
      "Loss = 0.16883598268032074\n",
      "Loss = 0.16676071286201477\n",
      "Loss = 0.1647530198097229\n",
      "Loss = 0.16280904412269592\n",
      "Loss = 0.16092534363269806\n",
      "Loss = 0.15909868478775024\n",
      "Loss = 0.15732617676258087\n",
      "Loss = 0.1556049883365631\n",
      "Loss = 0.15393273532390594\n",
      "Loss = 0.1523069590330124\n",
      "Loss = 0.1507255882024765\n",
      "Loss = 0.14918656647205353\n",
      "Loss = 0.14768801629543304\n",
      "Loss = 0.14622819423675537\n",
      "Loss = 0.14480550587177277\n",
      "Loss = 0.1434183418750763\n",
      "Loss = 0.14206533133983612\n",
      "Loss = 0.1407451182603836\n",
      "Loss = 0.13945648074150085\n",
      "Loss = 0.1381980925798416\n",
      "Loss = 0.13696900010108948\n",
      "Loss = 0.13576799631118774\n",
      "Loss = 0.1345941573381424\n",
      "Loss = 0.13344648480415344\n",
      "Loss = 0.13232411444187164\n",
      "Loss = 0.13122613728046417\n",
      "Loss = 0.13015171885490417\n",
      "Loss = 0.12910011410713196\n",
      "Loss = 0.12807053327560425\n",
      "Loss = 0.12706229090690613\n",
      "Loss = 0.12607461214065552\n",
      "Loss = 0.12510693073272705\n",
      "Loss = 0.12415851652622223\n",
      "Loss = 0.12322883307933807\n",
      "Loss = 0.12231723219156265\n",
      "Loss = 0.1214231550693512\n",
      "Loss = 0.12054607272148132\n",
      "Loss = 0.11968537420034409\n",
      "Loss = 0.11884064972400665\n",
      "Loss = 0.11801137030124664\n",
      "Loss = 0.11719702929258347\n",
      "Loss = 0.1163971871137619\n",
      "Loss = 0.11561138927936554\n",
      "Loss = 0.11483921855688095\n",
      "Loss = 0.1140802875161171\n",
      "Loss = 0.11333411186933517\n",
      "Loss = 0.11260036379098892\n",
      "Loss = 0.1118786409497261\n",
      "Loss = 0.11116864532232285\n",
      "Loss = 0.1104699969291687\n"
     ]
    }
   ],
   "source": [
    "# Run the computational graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        # Softmax\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Cross entropy to quantify how far off the predictions are\n",
    "        cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices = 1)\n",
    "\n",
    "        # Training loss\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "        # Learning rate\n",
    "        learning_rate = 0.8\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        _, l = sess.run(\n",
    "            [optimizer, loss],\n",
    "            feed_dict = {features: train_features, labels: train_labels})\n",
    "\n",
    "        print(\"Loss = {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: Batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(features) == len(labels)\n",
    "    \n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "    return(output_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch    \n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict = {features: last_features, labels: last_labels})\n",
    "    \n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict = {features:  valid_features, labels: valid_labels})\n",
    "    \n",
    "    print(\"Epoch: {:<4} - Cost: {:<8.3} Accuracy: {:5.3}\".format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
